% !TeX root = ../paper.tex
% !TeX encoding = UTF-8
% !TeX spellcheck = en_US


\section[Performance and Memory Overhead Experiments]{Performance and Memory Overhead Experiments}\label{sec:experiments}

  We present a short evaluation of the proof-of-concept framework implementation with regard to query performance.
  We also measure the memory overhead introduced by storing data in possibly hundreds of thousands of \glspl{dactor}, each storing only a relatively small amount of data respectively. 
  All tests were executed on a single consumer computer fitted with an Intel Core~i5-7600K CPU running at 3.8~GHz and 16~GB of DDR4 RAM.
  The test system runs the 64~bit version of the Kubuntu~18.10 Linux distribution.

  We performed experiments using an exemplary actor database system, which is modeled based on a real-world scenario.
  It consists of four different \gls{dactor} types, each containing one to three relations.
  The data stored in one \gls{dactor} ranges from seven to about 700 kilobytes depending on its type.
  We used a script to generate four different datasets emulating the scaling of the system by increasing the number of \gls{dactor}-instances and keeping the data size stored in one \gls{dactor} nearly constant.
  The script creates various primitive data types, such as \code{String}, \code{Double}, and \code{Int}, as well as complex data types, such as \code{ZonedDateTime}, and distributes the data across \glspl{dactor} and relations.
  The dataset sizes are reported in \cref{tab:memory_overhead}.

  For all runtime performance experiments the median runtime of $N=1000$ concurrently executed queries are reported.
  A point-query for data contained in a single instance of a \gls{dactor} object presents a response time latency of XX~ms.
  A concurrent insert of related data points into two \glspl{dactor} objects, managed by a concurrent functor ensuring a consistency constraint, runs XX~ms.

  For each dataset we performed three different tests in order to evaluate the memory overhead introduced by encapsuling the system's data in a large number of \glspl{dactor} and relations:
  \begin{description}
    \item[\textit{Single string}] Convert all data into its \code{String} representation and load it as a single big \code{String} into memory.
    This test serves as baseline for the other tests.
    \item[\textit{Relations}] Load the data into their respective \code{Relation}s, preserving the type information and using the in-memory data storage objects from our framework.
    \item[\textit{Framework}] Use the full-fledged framework to load the data into memory.
    This approach stores the data distributed across \glspl{dactor} in \code{Relation} objects.
  \end{description}

  To obtain the used memory of the objects in our test approaches, we used VisualVM\footnote{\url{https://visualvm.github.io/}} to create heap dumps.
  After the data was completely loaded into memory, we triggered a garbage collection run and created a heap dump.
  VisualVM is able to compute the retained sizes of object hierarchies in those dumps.
  This allowed us to investigate the memory usage of selected objects and their members in detail.
  We report the results of those experiments in \cref{tab:memory_overhead}.
  
  \begin{table}
    \centering
    \begin{tabular}{crrrrrr}
      \toprule
      \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Disk size}} & \multirow{2}{*}{\textbf{\# \Glspl{dactor}}} & \multicolumn{3}{c}{\textbf{Heap size}} & \textbf{Overhead /}\\
      & & & \textbf{\textit{Single string}} & \textbf{\textit{Relations}} & \textbf{\textit{Framework}} & \textbf{\gls{dactor}}\\
      \midrule
      $D_1$ &  10~MB &   829 &  11~MB &  28~MB &  29~MB & 526~B \\
      $D_2$ &  25~MB & 2~578 &  18~MB &  43~MB &  44~MB & 539~B \\
      $D_3$ &  50~MB & 4~373 &  47~MB & 116~MB & 119~MB & 532~B \\
      $D_4$ & 100~MB & 8~618 & 101~MB & 233~MB & 237~MB & 534~B \\
      % Average overhead = 533B
      \bottomrule& 
    \end{tabular}
    \caption{Used heap size of our three different methods to load data into memory and the memory overhead of \glspl{dactor} compared across the four datasets.}
    \label{tab:memory_overhead}
  \end{table}

  If the data is loaded into memory as a single big \code{String} object, it takes up around the same amount of heap as the dataset is big.
  Storing the data in \textit{Relations} introduces a overhead of about $150 \%$.
  % relations - string / string = ratio
  % 18162800 / 11315812 = 161%
  % 26453405 / 18637898 = 142%
  % 72695967 / 49246330 = 148%
  % 145392010 / 98492584 = 148%
  Even for the smallest dataset the data is split up across thousands of relations, which each use a two dimensional \code{Array} to store the individual values in a non-optimized way.
  In addition to that, relations also store metadata about the contained data, such as column names and data types.
  % show relations with more data stored than a couple kilobytes
  As we can see in \cref{tab:memory_overhead}, using the full framework with \glspl{dactor} does not introduce much additional memory overhead.
  On average, using a \gls{dactor} only needs an additional 533~B more.
  
  Let us assume that we have a 1~TB database and we chose to store 1~MB per \gls{dactor}.
  This requires the actor database to instantiate about one million \glspl{dactor}.
  Using the average overhead of 550~B per \gls{dactor}, this would yield a relative memory overhead of only $0.05~\%$.
  Doing the same thought experiment with storing 10~MB per \gls{dactor}, results in about 100~000 \glspl{dactor} and reduces the relative memory overhead to $0.005 \%$.
