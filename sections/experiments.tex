% !TeX root = ../paper.tex
% !TeX encoding = UTF-8
% !TeX spellcheck = en_US


\section[Dactor Memory Overhead Experiments]{\Gls{dactor} Memory Overhead Experiments}\label{sec:experiments}

  In our framework, a complex query comprising multiple \glspl{functor} has a runtime of 25~ms while queries involving a single \gls{dactor} instance respond under 10~ms.
  Our concept is based on the usage of hundreds of thousands of \glspl{dactor}, which all store only a small amount of data.
  Therefore, the question of how much memory overhead is introduced for storing data split across a multitude of actors is more interesting than the computation time.

  We performed experiments using an exemplary actor database system, which is modeled based on a real-world scenario.
  It consists of four different \gls{dactor} types, each containing one to three relations.
  The data stored in one \gls{dactor} ranges from seven to about 700 kilobytes depending on its type.
  This small data size makes the relative memory overhead more visible as we need thousands of \glspl{dactor} and relations to load our datasets into memory.
  We used a script to generate four different datasets emulating the scaling of the system by increasing the number of \gls{dactor}-instances and keeping the data size stored in one \gls{dactor} nearly constant.
  The script creates various primitive data types, such as \code{String}, \code{Double}, and \code{Int}, as well as complex data types, such as \code{ZonedDateTime}, and distributes the data across \glspl{dactor} and relations.
  The dataset sizes are reported in \cref{tab:memory_overhead}.

  For each dataset we performed three different tests:
  \begin{description}
    \item[\textit{Single string}] Convert all data into its \code{String} representation and load it as a single big \code{String} into memory.
    This test serves as baseline for the other tests.
    \item[\textit{Relations}] Load the data into their respective \code{Relation}s, preserving the type information and using the in-memory data storage objects from our framework.
    \item[\textit{Framework}] Use the full-fledged framework to load the data into memory.
    This approach stores the data distributed across \glspl{dactor} in \code{Relation} objects.
  \end{description}

  To obtain the used memory of the objects in our test approaches, we used VisualVM\footnote{\url{https://visualvm.github.io/}} to create heap dumps.
  After the data was completely loaded into memory, we triggered a garbage collection run and created a heap dump.
  VisualVM is able to compute the retained sizes of object hierarchies in those dumps.
  This allowed us to investigate the memory usage of selected objects and their members in detail.
  We report the results of those experiments in \cref{tab:memory_overhead}.
  
  \begin{table}
    \centering
    \begin{tabular}{crrrrrr}
      \toprule
      \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Disk size}} & \multirow{2}{*}{\textbf{\# \Glspl{dactor}}} & \multicolumn{3}{c}{\textbf{Heap size}} & \textbf{Overhead /}\\
      & & & \textbf{\textit{Single string}} & \textbf{\textit{Relations}} & \textbf{\textit{Framework}} & \textbf{\gls{dactor}}\\
      \midrule
      $D_1$ &  10~MB &   829 &  11~MB &  28~MB &  29~MB & 526~B \\
      $D_2$ &  25~MB & 2~578 &  18~MB &  43~MB &  44~MB & 539~B \\
      $D_3$ &  50~MB & 4~373 &  47~MB & 116~MB & 119~MB & 532~B \\
      $D_4$ & 100~MB & 8~618 & 101~MB & 233~MB & 237~MB & 534~B \\
      % Average overhead = 533B
      \bottomrule& 
    \end{tabular}
    \caption{Used heap size of our three different methods to load data into memory and the memory overhead of \glspl{dactor} compared across the four datasets.}
    \label{tab:memory_overhead}
  \end{table}

  If the data is loaded into memory as a single big \code{String} object, it takes up around the same amount of heap as the dataset is big.
  Storing the data in \textit{Relations} introduces a overhead of about $150 \%$.
  % relations - string / string = ratio
  % 18162800 / 11315812 = 161%
  % 26453405 / 18637898 = 142%
  % 72695967 / 49246330 = 148%
  % 145392010 / 98492584 = 148%
  Even for the smallest dataset the data is split up across thousands of relations, which each use a two dimensional \code{Array} to store the individual values in a non-optimized way.
  In addition to that, relations also store metadata about the contained data, such as column names and data types.
  % show relations with more data stored than a couple kilobytes
  As we can see in \cref{tab:memory_overhead}, using the full framework with \glspl{dactor} does not introduce much additional memory overhead.
  On average, using a \gls{dactor} only needs an additional 533~B more.
  
  Let us assume that we have a 1~TB database and we chose to store 1~MB per \gls{dactor}.
  This requires the actor database to instantiate about one million \glspl{dactor}.
  Using the average overhead of 550~B per \gls{dactor}, this would yield a relative memory overhead of only $0.05~\%$.
  Doing the same thought experiment with storing 10~MB per \gls{dactor}, results in about 100~000 \glspl{dactor} and reduces the relative memory overhead to $0.005 \%$.
