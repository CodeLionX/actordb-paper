% !TeX root = ../paper.tex
% !TeX encoding = UTF-8
% !TeX spellcheck = en_US

\section{Domain Actor Database Framework}\label{sec:framework}
  After describing the general concept of an \textit{Actor Database System}, we present the implementation of a proof-of-concept application development framework, which allows for the definition of an application's data model and the provisioning of corresponding \glspl{dactor} as part of the application runtime.

  \subsection{Domain Actor Design}\label{subsec:domain_actor_design}
    To illustrate the definition of \code{Dactor}s, we use the example introduced in \cref{sec:dactors}, which is schematically depicted in \cref{fig:film_diagram}.
    The corresponding definition of the \code{Film}'s data model is shown in \cref{lst:film_definition}.
    Developers can model the application's domain objects by defining \gls{dactor} types as subclasses of the framework-provided \code{Dactor} class in a declarative way.
    Instances of such user-defined \gls{dactor} types are managed by the framework and are available for messaging in a consistent namespace.
    Using the column's predefined data types, all functions support compile-time type-safety.
    Due to the \gls{dactor} system sharing the application's runtime and programming environment, these data or object types are equal to the types handled in any application logic.
    Thus, this approach helps eliminate the impedance mismatch between application logic and data tier with regard to handled data types and object (de-)serialization.
  
  \subsection[Multi-Dactor Queries]{Multi-\Gls{dactor} Queries}\label{subsec:multi_dactor_queries}
    Explicit message handling in \glspl{dactor} is used to implement the cascading communication pattern described in Section~\ref{sec:concept}.
    Besides this, \glspl{functor} are the framework's system, which enable inter-\gls{dactor} communication and computations.
    \Glspl{functor} encapsulate the processing of a high-level request in a new, short-living actor.
    They are able to communicate with other \glspl{dactor}, track the state of pending requests and handle failure cases.
    Every actor can create a new \gls{functor} to encapsulate multiple depending requests to \glspl{dactor}.
    The \gls{functor} handles the message processing and sends the final result or a failure message back to its parent actor before stopping itself.
    \Glspl{functor} are always called from an Akka actor as a child actor.
    This Akka-specific hierarchical relationship enables notifying the calling actor even in case of an unforeseen crash of the \gls{functor} themselves, which in turn allows to trigger error handling, e.g. retrying the \gls{functor} execution.
    
    Returning to the web application example, let us consider, we want to add a new film to the database.
    This would involve changes to a new \code{Film} and the corresponding \code{Studio} \glspl{dactor} instances.
    We can combine the concurrent and sequential \glspl{functor} to implement this functionality, which is displayed in \cref{fig:functor_diagram}.
    Both sequential \glspl{functor} are comprised of two subsequent steps:
    They retrieve information from one \glspl{dactor} to update the other one.
    They are independent of each other, so they can be executed in parallel, which is done by using the concurrent \gls{functor}.
    It only sends a successful response to its caller after both sequential \glspl{functor} have sent their responses to the concurrent \gls{functor}.
    
    \begin{figure}
      \centering
      \includestandalone[width=\textwidth]{pictures/tikz/functor_diagram}
      \caption{Component diagram indicating the message flow through \gls{functor} objects and their supervision by the calling actor or \gls{dactor}. Arrow and dashed arrow pairs indicate corresponding request and response messages. The outgoing requests of each sequential \gls{functor} are numbered to indicate their order.}
      \label{fig:functor_diagram}
    \end{figure}

    \textcolor{red}{Describe, what is handled by our framework and what not.}

  \subsection{Framework Discussion}\label{subsec:framework_discussion}
    In this section we discuss three core problems, which pose challenges in a distributed setting and how our framework addresses these:
    
    \textbf{Data partitioning} in an actor database system differs fundamentally from common partitioning techniques used in relational databases.
    While large tables are typically partitioned based on a specific column's value or the hash thereof, our framework provides \glspl{dactor} as entities for data encapsulation and partitioning.
    \Glspl{dactor} can be provisioned across multiple virtual runtimes and physical machines, because every \gls{dactor} instance is independent of the others and the only mean of communication is message passing.
    As such, they provide flexible, fine-grained data partitioning based on application needs.
    
    From the distributed setting of the database system the new problem of partition or \textbf{actor discovery} arises.
    The framework maintains a unified namespace, in which each \gls{dactor} instance is identified by its \gls{dactor} type and a unique id.
    In fact, querying a specific \gls{dactor} just requires obtaining the messaging address from the name-service and sending a message to it.
    In the case of a multi-node deployment, this is complemented by the functionality of Akka Cluster Sharding~\cite{akka:clustersharding}, which routes the messages to the right physical host.
    
    Finally, \textbf{failure handling}, especially with regard to computations relying on multiple \glspl{dactor}' data, requires careful monitoring due to \gls{dactor} distribution.
    Building on the Akka framework's parent-child supervision concept, our framework allows for transparent failure handling configurations.
    Failures can be handled within \glspl{dactor} if appropriate.
    In case of multi-\gls{dactor} queries a fail-fast approach is chosen to allow calling actors or \glspl{dactor} to react to exceptions in a timely manner.
