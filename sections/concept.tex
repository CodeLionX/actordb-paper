% !TeX root = ../paper.tex
% !TeX encoding = UTF-8
% !TeX spellcheck = en_US

\section{Domain Actor Database Concept}\label{sec:concept}
  Our concept is based on the idea of actor database systems as introduced by~\citeauthor{manifesto}~\cite{manifesto}.
  Database systems should be a distributed runtime and provide the appropriate programming abstraction.
  This is solved by actor database systems.
  The combination of application and data tier provides considerable advantages for data-centric systems.
  This architecture allows leveraging domain and application knowledge to dynamically model the data layout, especially concerning data partitioning and replication schemes.
  This makes the database system modular, cloud-ready and scalable.
  % add here: why is it better to use domain knowledge to partition and distribute data?

  \subsection{Domain Actors}\label{sec:dactors}
    Similar to \citeauthor{Shah:reactdb}~\cite{Shah:reactdb}, we introduce a special type of actor, called Domain Actor (\gls{dactor} for short), that acts as an application-defined scaling unit.
    \glspl{dactor} can be used to model application-domain objects and encapsulate the object's state and application logic in an actor.
    Using actors for this enforces technical encapsulation of state access due to the purely private state in actors and the need of explicit asynchronous messaging between the actors.
    This makes it easier to reason about state changes, bugs and other failures, as only code within the \gls{dactor} can change the corresponding state.

    As \glspl{dactor} not only contain data, but also the corresponding domain logic, computation is executed concurrently.
    This supports designing a modular and extensible database system and improves scalability.
    The system grows naturally based on the load.
    Take an e-commerce application as an example for that:
    If we have a customer entity for every customer in our system, then we can model each entity as a \gls{dactor}.
    This allows our system to scale linearly with the number of customers using it, because each customer is represented with their own \gls{dactor}, which holds their information and performs computations.
    This also illustrates that such a system tends to be more robust compared to monolithic systems, because if customer A's \gls{dactor} crashes for an unknown reason, all other customer \glspl{dactor} are unaffected.

    Actors provide single-threaded semantics, which makes enforcing constraints on data tied to one domain object stored inside \glspl{dactor} no issue.
    It is implemented via application logic within the \gls{dactor}.
    State querying and modification within \glspl{dactor} is possible in a declarative way, but communication across all kind of actors is explicitly defined via asynchronous messages and how the \glspl{dactor} handle the said messages.

  \subsection{Communication between Domain Actors}
    Not all computation can be done with the information residing in a single \gls{dactor}.
    Communication between \glspl{dactor} is required.
    As already mentioned, inter-\gls{dactor} communication is realized via explicit asynchronous message passing.
    This allows application developers to chose the right messaging pattern for their use case.
    The choice is heavily influenced by the data layout used for the \glspl{dactor} and sets the level of parallelization and the network-load for the computation.

    We consider three main messaging patterns for inter-\gls{dactor} communication, which are shown in figure~\ref{fig:comp_patterns}: Cascading Computation, Sequential Computation, Concurrent Computation.
    They all assume a request-response messaging schema and take a high-level message to a \gls{dactor}, lets call it \gls{dactor} A, as starting point.
    
    \begin{enumerate}
      \item\label{enum:comp_pattern_1} \textbf{Cascading Computation}
        A high-level message to \gls{dactor} A, will trigger successive messages to other \glspl{dactor}, which are hidden from the original requester.
        Depending on the computation performed in \gls{dactor} A, it can send requests to another \gls{dactor} (e.\,g. \gls{dactor} B) and handle the result itself before returning the response to the caller.
        \gls{dactor} B can itself send messages to other \glspl{dactor} as well.
        As one can see in figure~\ref{fig:comp_pattern_1}, this pattern is comparable to function calls in \gls{oop}, but can lead to a growing cascade of messages sent and received.
        As each message can introduce network latency, it can take significantly more time to receive the top-level response compared to the situation when all computation is done in \gls{dactor} A.
        
        - dactors need to hold state for pending responses for failure management and intermediate results, original requester
        - makes writing code complex and error-prone
        + similar to \gls{oop}
        + high separation of concerns, when tell-don't-ask is used
        
        --> good when used sparsely, for small cascades
        
      \item\label{enum:comp_pattern_2} \textbf{Sequential Computation}
        Figure~\ref{fig:comp_pattern_2}
        - serialized computation chain (sometimes required by application)
        - extra actor must be created for each top-level request (overhead can even be reduced with pools, (may already be used in Akka))
        + intermediate state is managed in functor
        + \gls{dactor} A does not need to retain pending responses, free for other requests
        + simple logic in functor as it only processes one request chain, then dies
        + simple logic in \gls{dactor} (only state changes and forwarding of computation to functor
        
      \item\label{enum:comp_pattern_3} \textbf{Concurrent Computation}
        Figure~\ref{fig:comp_pattern_3}
        pros / cons similar to \ref{enum:comp_pattern_2}
        - none?
        + highly parallelized computation
    \end{enumerate}


    \begin{figure}
      \centering

      \begin{subfigure}[c]{0.49\textwidth}
        \includestandalone[width=\textwidth]{pictures/tikz/cascading_computation}
        \subcaption{Cascading Computation}
        \label{fig:comp_pattern_1}
        \hfill\\
        \includestandalone[width=\textwidth]{pictures/tikz/concurrent_computation}
        \subcaption{Concurrent Computation}
        \label{fig:comp_pattern_3}
      \end{subfigure}
      \begin{subfigure}[c]{0.5\textwidth}
        \includestandalone[width=\textwidth]{pictures/tikz/sequential_computation}
        \subcaption{Sequential Computation}
        \label{fig:comp_pattern_2}
      \end{subfigure}
      \caption{Inter-\gls{dactor}-communication patterns. Dotted lines show actor lifetime and gray bars indicate that an actor performs computation or holds state that is related to the showed message flow.}
      \label{fig:comp_patterns}
    \end{figure}


    Introducing functors for \ref{enum:comp_pattern_2} and \ref{enum:comp_pattern_3} \dots
    % cameo pattern: http://blog.simplex.software/the-cameo-pattern


  \subsection{Domain Actor Database System}\label{sec:domain_actor_database}
    A domain actor database system is the combination of \glspl{dactor} and other types of actors, which together offer an interface to other applications or clients.
    The domain actor database system is a distributed runtime for persisting data and performing computations on it.
    Each \gls{dactor} must incarnate a specific \gls{dactor} type.
    This type is defined by application developers and determines the data schema encapsulated by the \gls{dactor} and the messages it can handle (its behavior).
    \glspl{dactor} are the only place in the system that actually have persistent state.
    Other actors are stateless or have non-durable state, such as intermediate results or actor references.
    These actors can interact with the \glspl{dactor} via the defined messages and represent application services or components of the infrastructure layer.
    
    To develop such a system, we can decompose our application domain into encapsulated entity types.
    Those types can be modeled as \glspl{dactor} and consist of a data schema and behavior.
    During this process we have to keep our requirements regarding query patterns and scaling in mind.
    Those requirements will influence the domain decomposition and the messaging patterns used.
    The entry points to our application are stateless actors on top of the \glspl{dactor}, which provide high throughput and can be scaled out easily.
    They act as routers for incoming requests and perform computations on a higher level.

    \begin{itemize}
      \item Consistency?
      \item Transactional guarantees?
    \end{itemize}

    % akka: (i would put this in 4.)
    % message-passing
    % supervision

    % im framework:
    % beispiel


    % how actors solve the impedance mismatch
    % "passt auf, dass hier eure Idee im Vordergrund steht und ihr nicht auf andere Gebiete ausschweift"
    % Computation parallelization on a domain-level object basis
    % Modularity, Testability
